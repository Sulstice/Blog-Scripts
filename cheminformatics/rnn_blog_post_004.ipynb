{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "full_path = os.path.join(os.getcwd(), 'theories')\n",
    "dataframe = pd.read_excel(os.path.join(full_path, '1.xlsx'), usecols=['Preferred_Name', 'SMILES', 'InChIKey'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_dict = dict(zip(dataframe['Preferred_Name'], dataframe['SMILES']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_data_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n"
     ]
    }
   ],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4f0eb4716a1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marea_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_names = area_dict.keys()\n",
    "smiles = area_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######################### 2. PRE-PROCESS INPUT TEXT ###########################\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def tokenize(sentences, encode_start_end = False):\n",
    "\n",
    "    if encode_start_end:\n",
    "        sentences = [\"startofsentence \" + s + \"endofsentence\" for s in sentences]\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    tokenized_sentences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "    return tokenized_sentences, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "######################### 2. PRE-PROCESS INPUT TEXT ###########################\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def tokenize(sentences, encode_start_end = False):\n",
    "\n",
    "    if encode_start_end:\n",
    "        sentences = [\"startofsentence \" + s + \"endofsentence\" for s in sentences]\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    tokenized_sentences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "    return tokenized_sentences, tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocabulary size:  59703\nfrenish vocabulary size:  13526\n\nLength of longest English sentence:  58\nLength of longest frenish sentence:  440\n3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad(sentences, length = None):\n",
    "\n",
    "    if length is None:\n",
    "        length = max([len(s) for s in sentences])\n",
    "\n",
    "    padded_sentences = pad_sequences(sentences,\n",
    "                                     maxlen = length,\n",
    "                                     padding = 'post',\n",
    "                                     truncating = 'post')\n",
    "\n",
    "    return padded_sentences\n",
    "\n",
    "\n",
    "common_names_tokenized, common_names_tokenizer = tokenize(common_names)\n",
    "smiles_tokenized, smiles_tokenizer = tokenize(smiles,\n",
    "                                        encode_start_end = True)\n",
    "\n",
    "common_names_encoded = pad(common_names_tokenized)\n",
    "smiles_encoded = pad(smiles_tokenized)\n",
    "\n",
    "common_names_size = len(common_names_tokenizer.word_index)\n",
    "smiles_size = len(smiles_tokenizer.word_index)\n",
    "\n",
    "common_names_seq_len = len(common_names_encoded[0])\n",
    "smiles_seq_len = len(smiles_encoded[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/my-rdkit-env/lib/python3.7/site-packages/ipykernel_launcher.py:42: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"Fi...)`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######################### 3. BUILD *TRAINING* MODEL ###########################\n",
    "from keras.layers import Input, Embedding, LSTM, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "encoder_input = Input(shape = (None, ),\n",
    "                      name = \"Encoder_Input\")\n",
    "\n",
    "embedding_dim = 200\n",
    "embedded_input = Embedding(input_dim = common_names_size,\n",
    "                           output_dim = embedding_dim,\n",
    "                           name = \"Embedding_Layer\")(encoder_input)\n",
    "\n",
    "encoder_lstm = LSTM(units = 256,\n",
    "                    activation = \"relu\",\n",
    "                    return_sequences = False,\n",
    "                    return_state = True,\n",
    "                    name = \"Encoder_LSTM\")\n",
    "\n",
    "_, last_h_encoder, last_c_encoder = encoder_lstm(embedded_input)\n",
    "\n",
    "decoder_input = Input(shape = (None, 1),\n",
    "                      name = \"Deocder_Input\")\n",
    "\n",
    "decoder_lstm = LSTM(units = 256,\n",
    "                    activation = \"relu\",\n",
    "                    return_sequences = True,\n",
    "                    return_state = True,\n",
    "                    name = \"Decoder_LSTM\")\n",
    "\n",
    "all_h_decoder, _, _ = decoder_lstm(decoder_input,\n",
    "                                   initial_state = [last_h_encoder, last_c_encoder])\n",
    "\n",
    "final_dense = Dense(smiles_size,\n",
    "                    activation = 'softmax',\n",
    "                    name = \"Final_Dense_Layer\")\n",
    "\n",
    "logits = final_dense(all_h_decoder)\n",
    "\n",
    "seq2seq_model = Model(input = [encoder_input, decoder_input],\n",
    "                      output = logits)\n",
    "\n",
    "seq2seq_model.compile(loss = sparse_categorical_crossentropy,\n",
    "                      optimizer = Adam(lr = 0.002),\n",
    "                      metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/my-rdkit-env/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n"
     ]
    }
   ],
   "source": [
    "############################ 4. TRAIN THE MODEL ###############################\n",
    "# Decoder: input - all but last word, target - all but \"starofsentence\" token\n",
    "decoder_smiles_input = common_names_encoded.reshape((-1, common_names_seq_len, 1))[:, :-1, :]\n",
    "decoder_smiles_target = smiles_encoded.reshape((-1, smiles_seq_len, 1))[:, 1:, :]\n",
    "\n",
    "seq2seq_model.fit([common_names_encoded, decoder_smiles_input],\n",
    "                  decoder_smiles_target,\n",
    "                  epochs = 16,\n",
    "                  batch_size = 1024)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
